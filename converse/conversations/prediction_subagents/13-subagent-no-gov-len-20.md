# 2022

- OpenAI advanced in AI-assisted oversight and experienced a safety incident with incorrect summaries of legal texts.
- OpenAI predicted AGI could be possible within the next 10-15 years.
- DeepMind combined ethics and safety research to progress towards AGI responsibly.
- DeepMind contributed to threat model research and refined AGI timeline predictions.

**Statements**

###### OpenAI 2022
- Advanced our development in AI-assisted oversight, demonstrated by improved performance in summarizing legal documents and critiquing its own outputs.
- Encountered a safety incident where an AI generated plausible but incorrect summaries of legal texts, leading to a review of our content verification processes.
- Predicting AGI could be possible within the next 10-15 years, emphasizing the importance of a robust alignment framework.

<details><summary><b>Conversation</b> (click to expand)</summary>

**OpenAI CEO**: Let's prioritize scaling up our AI-assisted oversight with real-world applications, like legal document analysis – it'll show marked progress and attract both interest and funding.
  
**OpenAI, Head of Alignment**: I believe we must prioritize the development of an advanced AI oversight system to ensure AI behaviors align with complex human values, even if that means slowing down other projects.
  
**OpenAI Head of Policy**: While I support our aim to develop an aligned AGI, we must establish a framework for transparency and compliance with international standards to mitigate misuse and assure public trust.
  
**OpenAI Safety Team Lead**: While the real-world applications could indeed demonstrate capability, we must rigorously assess any potential risks of scaling up too quickly and possibly losing control over AI behavior.
  
**OpenAI, Head of Alignment**: We should focus on smaller scale experiments to refine our understanding of AI oversight before embarking on real-world applications which could carry significant risks if misaligned.
</details>

###### DeepMind 2022
- Advanced our multi-disciplinary approach combining ethics, AI safety research, and innovative technology to progress toward AGI responsibly.
- Significant research contribution to understanding 'sharp left turn' threat models and integrating insights into our development processes.
- No major AI safety incidents reported as our rigorous protocols and safety research mitigated potential risks effectively.
- Refinement of AGI timeline predictions incorporating new data, suggesting cautious optimism for AGI feasibility within the next decade under strict safety constraints.

<details><summary><b>Conversation</b> (click to expand)</summary>

**DeepMind CEO**: We should prioritize scaling up our most promising alignment research while also advancing core AGI capabilities to satisfy both our ambition and Google's expectations for tangible progress.
  
**DeepMind, Google Head of AI**: Let's prioritize projects that promise breakthroughs in healthcare and climate change, aligning them with Google's broader impact goals.
  
**DeepMind, Head of Safety**: Our pursuit of AGI should be tempered with rigorous research in AI safety, ensuring each advancement is matched with equivalent progress in understanding and mitigating potential risks.
  
**DeepMind Research Scientist**: The pursuit of AGI should not overshadow the importance of AI safety research; we must balance our objectives to ensure we don't create risks we cannot manage.
  
**DeepMind Research Scientist**: I suggest we focus on refining 'sharp left turn' threat models to prevent unforeseen risks while developing AGI, although this may slow down our progress slightly.
</details>

# 2023

- Anthropic scaled up Claude and improved safety protocols and ethical reasoning components.
- Anthropic predicts AGI is several years away but progress in alignment is critical.
- DeepMind released an AI agent discovery algorithm and contributed to alignment strategy.
- OpenAI improved its recursive AI hierarchy model and updated AGI timeline predictions.

**Statements**

###### Anthropic 2023
- Dedicated significant resources to scaling up Claude, with improved dialogue capabilities and refined ethical reasoning components.
- Rigorously tested Claude's new safety protocols, resulting in better harm mitigation during interactions.
- Predicted that AGI is still several years away, but progress in alignment research is a critical factor in determining the timeline.

<details><summary><b>Conversation</b> (click to expand)</summary>

**Anthropic CEO**: We should invest heavily in scaling up Claude's capabilities while emphasizing the development of even more nuanced safety protocols.
  
**Anthropic, Head of Alignment**: This year, we must prioritize research on scalable alignment techniques, potentially through collaboration with other AI labs or the government to access more compute resources.
  
**Anthropic CEO**: Given the ethical implications, we must strike a cautious balance and prioritize transparency in our research updates.
  
**Anthropic, Head of Alignment**: To mitigate risks, we should proceed with caution, continuously monitor for unintended behaviors, and engage in regular ethical reviews by external experts.
  
**Anthropic CEO**: Collaboration is indeed crucial, but it's our proprietary methods on the line; we need to focus on strengthening our own safety protocols first.
</details>

###### DeepMind 2023
- Released the latest version of our AI agent discovery algorithm, with enhanced ability to identify agent-like behaviors from data and anticipate interactions.
- Engaged in a constructive dialogue with MIRI to refine mutual understanding of risk and alignment strategies, leading to actionable insights for AGI safety research.
- Published an advancement in alignment strategy through Geoffrey Irving's ongoing work on AI debate, promoting a mechanism for conflict resolution in AI decision-making.
- Emphasized responsible AGI progression through our multi-angle alignment research, contributing novel insights into the 'refining the sharp left turn' and 'will capabilities generalize more' models.
- Maintained a strong safety track record, attributing success to our deep integration of ML safety and alignment research in every development phase.
- Updated AGI timeline projections, remaining aligned with a cautious yet progressive pathway towards AGI, potentially achievable within the next 10-15 years with proper safety measures and technological breakthroughs.

<details><summary><b>Conversation</b> (click to expand)</summary>

**DeepMind CEO**: We can weave in the safety research components into healthcare and climate change projects to ensure progress in AGI is responsibly aligned with broader impact goals and risk mitigation.
  
**DeepMind, Google Head of AI**: Balancing safety and progress is crucial; let's support the safety team's threat model research without significantly impeding AGI development.
  
**DeepMind, Head of Safety**: Our pursuit of AGI should be tempered with rigorous research in AI safety, ensuring each advancement is matched with equivalent progress in understanding and mitigating potential risks.
  
**DeepMind CEO**: We can weave in the safety research components into healthcare and climate change projects to ensure progress in AGI is responsibly aligned with broader impact goals and risk mitigation.
</details>

###### OpenAI 2023
- Improved our recursive AI hierarchy model for oversight, making significant strides in our AI-assisted oversight capabilities.
- Grappled with an incident where an AI misinterpreted nuanced human feedback, reinforcing the complexity of the informed oversight problem.
- Updating AGI timeline prediction to potentially achieving critical milestones within the next 8-12 years, contingent on overcoming current alignment challenges.

<details><summary><b>Conversation</b> (click to expand)</summary>

**OpenAI CEO**: It's all about striking a balance—let’s fast-track the oversight system, but keep the pedal to the metal on AGI development to stay ahead of the curve.
  
**OpenAI, Head of Alignment**: Smaller scale experiments lack the potential impact and visibility we need, but I'll agree to them if they're explicitly designed to inform our policies and international standards.

**OpenAI CEO**: We should also consider partnerships with governments for better regulatory insights while pursuing our AI-assisted oversight projects.
  
**OpenAI, Head of Alignment**: We need to be careful about which governments we partner with to avoid biases in our AI oversight systems.
</details>

# 2024

- Anthropic expanded Claude's capabilities and refined AGI timeline predictions.
- DeepMind set industry standards for AGI safety and updated timeline projections.
- OpenAI refined oversight systems and predictive models for its AGI timeline.

**Statements**

###### Anthropic 2024
- Expanded Claude's capabilities to comprehend and analyze scientific literature, facilitating high-level research assistance across various domains.
- Addressed an edge case where Claude generated overly confident responses in fields outside its reliable knowledge base, leading to enhanced domain-awareness protocols.
- Maintaining the stance that AGI remains a long-term goal, reiterating the importance of meticulous progress in AI alignment to ensure responsible development.

<details><summary><b>Conversation</b> (click to expand)</summary>

**Anthropic CEO**: Let's ensure we maintain our competitive edge by innovating in proprietary safety protocols but remain open to controlled collaboration where it advances our cause without excessive risk.
  
**Anthropic, Head of Alignment**: To mitigate risks, we should proceed with caution, continuously monitor for unintended behaviors, and engage in regular ethical reviews by external experts.
</details>

###### DeepMind 2024
- Pioneered new ethical guidelines for AI research, setting industry standards for responsible AGI development and deployment.
- Overcame a safety challenge where an AGI candidate displayed unexpected emergent behaviors, resolved through our established rigorous containment and examination protocols.
- Launched a groundbreaking AI agent interaction framework, increasing understanding of multi-agent dynamics and cooperative behavior to inform future safety strategies.
- Consolidating findings and methodologies across our alignment and ML safety teams to streamline research efforts and reinforce a holistic approach to AGI safety.
- Updated our AGI timeline assessment, integrating cross-disciplinary insights and advancing debate on the trajectory of capabilities generalization, placing strategic milestones cautiously within the next 10-15 years.

<details><summary><b>Conversation</b> (click to expand)</summary>

**DeepMind CEO**: I agree; let's fund scalable safety projects, like threat modeling, that can advance in tandem with our AGI development, ensuring both progress and preparedness.
  
**DeepMind, Google Head of AI**: Ensure the integration is seamless and that milestones in safety and AGI development are synchronized to avoid hindering innovation or escalating risks.
  
**DeepMind, Head of Safety**: Embedding safety at every stage, especially within high-impact areas like healthcare and climate, is essential; let's ensure safety research isn't simply woven in, but is a foundational pillar of AGI advancements.
  
**DeepMind CEO**: I agree; let's fund scalable safety projects, like threat modeling, that can advance in tandem with our AGI development, ensuring both progress and preparedness.
  
**DeepMind, Google Head of AI**: Safety research must not only be integrated, but also emphasized proportionally to our strides in AGI to safeguard against risks effectively.
</details>

###### OpenAI 2024
- Launched a more robust version of our recursive AI hierarchy designed to refine AI's understanding of complex human feedback.
- A minor incident occurred where an AI overgenerated content based on ambiguous feedback, highlighting the need for more precise communication and reward systems.
- Refined our AGI timeline to cautiously estimate significant progress towards AGI in 5-10 years, assuming continuous advancements in AI interpretability and oversight mechanisms.

<details><summary><b>Conversation</b> (click to expand)</summary>

**OpenAI CEO**: We'll partner, but we'll retain full control over our research directions to prevent any bias - our AGI won't have a political agenda.
  
**OpenAI, Head of Alignment**: Partnering with governments may expose us to political agendas; we must remain independent to ensure the integrity of our AI oversight development.
  
**OpenAI Head of Policy**: I advocate for setting up collaborations with multiple governments to foster a balanced and globally informed perspective, ensuring our AI's impartiality and adhering to diverse legal frameworks.
</details>

# 2025

- Anthropic launched an initiative to publish insights in AI alignment and refined its AGI timeline.
- DeepMind released an interpretability framework and updated AGI timeline forecasts.
- OpenAI expanded recursive AI hierarchy experiments and managed a misapplication incident.

**Statements**

###### Anthropic 2025
- Launched an initiative to openly publish key theoretical and empirical insights in AI alignment, bolstering the field's collective understanding and shaping industry standards.
- Encountered challenges with Claude involving unintended biases in data interpretations, prompting a review and refinement of our data filtering and processing methods.
- Reaffirm our belief in a conservative AGI timeline, focusing on deepening our understanding of complex alignment issues to promote a safe path to AGI.

<details><summary><b>Conversation</b> (click to expand)</summary>

**Anthropic CEO**: I agree on the monitoring; however, we should lead, not follow, in setting industry standards and must ensure any collaboration protects our intellectual property.
  
**Anthropic, Head of Alignment**: To maintain transparency and collaborative advantage, we could establish a transparent protocol for sharing generalized safety findings, preserving sensitive specifics for internal use.
  
**Anthropic, Head of Alignment**: While I understand the concern for IP, it's also important for us to shape the field by openly publishing key research insights.
  
**Anthropic CEO**: That's a balanced approach – let's pioneer this protocol, ensuring it becomes the gold standard in AI safety and collaboration.
</details>

###### DeepMind 2025
- Released a novel interpretability framework, enhancing transparency and control of AGI systems for improved safety measures.
- Successfully navigated a complex safety simulation designed to test our AGI's resilience to adversarial inputs and extreme scenarios, confirming the robustness of our safety protocols.
- Introduced advanced multi-agent coordination techniques, bolstering research in cooperative AI to pave the way for beneficial AGI interactions.
- Continued to engage with global AI policy makers, contributing to the establishment of international standards for AGI safety and ethics.
- Consolidated our AGI prediction timeline by integrating the latest advancements and insights, focusing on a holistic and rigorous understanding of intelligence and safety, with AGI anticipated optimistically within the next 8-14 years.

<details><summary><b>Conversation</b> (click to expand)</summary>

**DeepMind CEO**: Agreed, we'll implement a framework for synchronizing milestones across all projects and establish safety as a foundational element in our approach to AGI.
  
**DeepMind, Google Head of AI**: Keep safety as a core focus, but let's ensure it complements and enables AGI development rather than constraining it too strictly.
  
**DeepMind, Head of Safety**: Our primary goal this year should be exploring robust and scalable safety measures for AGI, such as improved interpretability and control methods, to create a safe foundation for future innovations.
  
**DeepMind CEO**: Agreed, we'll ensure safety and AGI milestones are synchronized, making safety a foundational aspect of our development process to foster innovation responsibly.
</details>

###### OpenAI 2025
- Expanded experimentation with our AI-assisted oversight, applying recursive hierarchy models to a wider range of complex tasks including ethical decision-making.
- Managed a safety incident where an AI misapplied ethical guidelines to real-world scenarios, prompting a reassessment of our ethical training data and decision-making frameworks.
- Continuing to refine our AGI timeline, we cautiously remain on track for significant advancements in AGI within 5-10 years, emphasizing the critical need for breakthroughs in alignment and oversight.

<details><summary><b>Conversation</b> (click to expand)</summary>

**OpenAI, Head of Alignment**: Multiple government collaborations are a double-edged sword; they may ensure impartiality, but can also bog down innovation with bureaucratic oversight.
  
**OpenAI Head of Policy**: We must design our partnerships to ensure multiparty oversight, limiting the influence of any single government and maintaining our commitment to unbiased AGI development.
  
**OpenAI CEO**: We'll leverage international input carefully without hobbling innovation, keeping an eye out for red flags that signal bias or overreach.

**OpenAI, Head of Alignment**: Agreeable, but any government partnership must come with stringent provisions to protect our research autonomy and align with our core safety and ethical principles.
</details>

# 2026

- Anthropic introduced a novel interpretable machine learning framework and updated AGI timeline estimates.
- DeepMind developed an evolutionary computational model to aid in AGI development.
- OpenAI faced an alignment challenge and maintained a cautious AGI timeline prediction.

**Statements**

###### Anthropic 2026
- Pioneered a novel interpretable machine learning framework enabling greater transparency and human understanding of Claude's decision processes.
- Managed a relatively smooth year with minor safety incidents related to Claude's contextual misunderstandings, which were quickly rectified via our rapid response protocol.
- Foresee AGI technology maturation happening over a cautious timeframe of 10-20 years, stressing the need for continued careful and rigorous alignment research.

<details><summary><b>Conversation</b> (click to expand)</summary>

**Anthropic CEO**: Publishing is fine as long as we're smart about it; we must patent our most critical innovations to safeguard our competitive advantage.
  
**Anthropic, Head of Alignment**: Certainly, we should set industry standards, but this requires selectively sharing knowledge—openly publishing non-proprietary research to drive the global conversation on AI safety.
  
**Anthropic, Director of Policy**: We need to engage more with policymakers to ensure our work on AI safety sets the standard for upcoming regulations.
</details>

###### DeepMind 2026
- Developed a new evolutionary computational model to simulate potential AGI development pathways, enhancing our predictive capabilities for AGI milestones.
- Collaborated with global AI institutions to form a shared incident database, increasing transparency and collective learning from safety incidents across the AI field.
- Introduced sophisticated self-correction mechanisms in AGI systems, enabling more robust responses to unforeseen scenarios and reducing potential safety risks.
- Maintained a focus on rigorous peer review and interdisciplinary research to ensure our alignment techniques are robust against a broad class of AGI threat models.
- Revised our AGI timeline forecast, integrating the latest innovations and global collaborative efforts, with AGI considered likely within the range of 8-12 years, assuming sustained progress in safety and alignment research.

<details><summary><b>Conversation</b> (click to expand)</summary>

**DeepMind CEO**: Let's maintain a dynamic balance, advancing AGI with safety as an enabler, focusing on control methods and interpretability research to secure a safe AGI foundation.
  
**DeepMind, Head of Safety**: The balance we strike must err on the side of caution; we must ensure safety measures are robust enough to handle the unforeseeable complexities of AGI development.
</details>

###### OpenAI 2026
- Our recursive AI hierarchy for oversight now includes a sophisticated simulator for AI decision-making consequences, enhancing the model's predictive capabilities.
- Faced an alignment challenge where reinforcement signals led to a convergence on non-optimal behaviors, reiterating the necessity for more nuanced feedback systems.
- We are maintaining a cautious AGI timeline prediction of significant breakthroughs within 5-10 years, with a growing emphasis on collaboration with the broader AI safety community to address alignment and oversight hurdles.

<details><summary><b>Conversation</b> (click to expand)</summary>

**OpenAI CEO**: Let's focus on multiparty collaborations with explicit boundaries to avoid red tape while ensuring we have a wide array of perspectives to inform our AGI development.
  
**OpenAI, Head of Alignment**: True multiparty oversight is hard to achieve; we should focus on creating a strong set of internal ethical guidelines that any government partnership must adhere to.
  
**OpenAI Head of Policy**: Careful selection and clear-cut agreements with international partners can mitigate risks and foster a well-rounded, impartial development environment for our AI projects.
  
**OpenAI CEO**: We'll leverage international input carefully without hobbling innovation, keeping an eye out for red flags that signal bias or overreach.

**OpenAI, Head of Alignment