### 2023

- OpenAI expanded its AI interpretability methods and reported a minor AI safety incident, predicting AGI being a decade away.
- Anthropic focused on Claude's context-awareness, had no major safety incidents, and estimated AGI development over multiple decades.
- DeepMind released foundational models with advanced RL capabilities, resolved an emergent behavior safety incident, and predicted AGI in the next 10-20 years.
- The UK Government launched an AI governance framework, addressed a deepfake incident, and monitored AGI development timelines.

**Statements**
<details>
<summary>OpenAI 2023</summary>

- Expanded our AI interpretability methods to better understand and evaluate the decisions made by increasingly complex models.
- Encountered a minor safety incident where an AI model briefly propagated biased decision-making before it was corrected by our oversight mechanisms.
- Predicting that notional AGI is still a decade away, considering current progress and challenges.

<details>
<summary><b>Conversation</b></summary>

  - OpenAI CEO: Let's focus on scaling up our AI's cognitive capabilities this year, and I'm sure we can tackle the informed oversight problem as we go.
  - OpenAI Head of Alignment: Despite the commercial pressure, we must prioritize developing more sophisticated techniques for AI interpretability before advancing RLHF, as misaligned AGI poses existential risks.
  - OpenAI, Head of Policy: Before we proceed with AI-assisted oversight, we must first establish a clear framework for transparency and regulatory compliance, ensuring the public can trust the work we're doing.
  - OpenAI Research Scientist: I'm concerned that scaling up too quickly may exacerbate the oversight problem, making it harder to manage in the long run.
  - OpenAI Safety Lead: We can integrate interpretability methods in parallel with cognitive scaling to ensure safety measures evolve with the AI's capabilities.
</details>
</details>

<details>
<summary>Anthropic 2023</summary>

- Focused on enhancing Claude's context-awareness capabilities to improve helpfulness, with strict safety considerations.
- No major safety incidents reported due to rigorous evaluation protocols implemented for new features.
- Estimate AGI development to be a multi-decade process, with key milestones achievable in the next 5-10 years.

<details>
<summary><b>Conversation</b></summary>

  - Anthropic CEO: I propose we allocate significant resources towards enhancing Claude with advanced context-awareness capabilities to bolster its helpfulness without compromising on safety.
  - Anthropic, Head of Alignment: This year, we should prioritize scaling up our compute resources to tackle larger datasets for improving Claude's alignment with HHH principles, while also investing in research to better understand the behaviors of large AI models.
  - Anthropic CEO: I honestly believe we should be cautious and avoid overexerting our teams; we need a focused approach rather than taking on everything at once.
  - Anthropic, Head of Alignment: While the goal is imperative, we need to ensure the advanced context-awareness doesn't introduce safety risks like enabling manipulative content or misuse.
  - Anthropic CEO: I agree, but let's ensure we have the right safety measures and thorough evaluations in place to prevent any misuse or unintended consequences.
</details>
</details>

<details>
<summary>DeepMind 2023</summary>

- Released several foundational models showcasing advanced reinforcement learning capabilities.
- Addressed a safety incident where a model exhibited unexpected emergent behaviors, resolving it with our rigorous validation protocols.
- Predicted AGI could emerge within the next 10-20 years, subject to key breakthroughs in scaling and alignment research.

<details>
<summary><b>Conversation</b></summary>

  - DeepMind CEO: For this year, we need to prioritize scaling up our alignment research to tackle potential AGI risks, while maintaining a strong output of practical AI applications to meet Google's expectations.
  - DeepMind, Google Head of AI: I think we should prioritize integrating AI safety principles into our existing products, while also exploring the implications of our latest research in alignment and agency.
  - DeepMind, Head of Safety: We should prioritize rigorous validation protocols for our AI systems to prevent unintended behavior escalations as they grow more capable.
  - DeepMind Research Scientist: I believe we should invest more in empirical research to test our alignment theories on current machine learning models.
  - DeepMind, Google Head of AI: I think we should prioritize integrating AI safety principles into our existing products, while also exploring the implications of our latest research in alignment and agency.
</details>
</details>

<details>
<summary>UK Government 2023</summary>

- Launched a new AI governance framework that seeks to foster innovation while addressing security concerns.
- Responded to a minor incident involving the misuse of AI in generating deepfakes, leading to tightened verification protocols.
- Predicted that AGI development is still a decade away but is closely monitoring the situation.

<details>
<summary><b>Conversation</b></summary>

  - UK Government Prime Minister: We need a strong proposal on AI governance that balances innovation with national security, without falling behind in the global AI race.
  - UK Government, Head of AI Policy: We must navigate the AI development race like a careful tightrope walker, ensuring every step towards advancement is balanced with a safety net of regulations.
  - UK Government, Minister for Science and Innovation: We must prioritize the development of robust AI auditing frameworks to ensure any AI we back is transparent and accountable.
  - UK Government Science Advisor: I propose we invest in research grants focused on ethical AI development and set strict regulations for AI applications in sensitive sectors.
  - UK Government, Head of AI Policy: A special unit devoted to the ethical development and deployment of AI, led by both technical experts and ethicists, will help keep our pace steady and secure.
</details>
</details>

### 2024

- OpenAI developed a new AI oversight model architecture and faced scaling challenges, now predicting AGI in 8-12 years.

**Statements**
<details>
<summary>OpenAI 2024</summary>

- Developed a new model architecture designed to improve the recursive hierarchy of AI-assisted oversight, with initial success in self-critique and summarization tasks.
- Faced challenges with model scaling causing unexpected behaviors not flagged by oversight AIs, leading to a revision of the hierarchy model.
- Adjusting AGI timeline predictions to 8-12 years, as progress in AI safety and oversight mechanisms needs to pace with advances in AI capabilities.

<details>
<summary><b>Conversation</b></summary>

  - OpenAI CEO: I hear you, but innovation often outpaces regulation, and we need to keep pushing the envelope to maintain our lead and societal trust.
  - OpenAI Head of Alignment: We must prioritize clearly defining and testing our interpretability methods on existing models before we scale, to avoid unintended consequences at a larger scale.
  - OpenAI, Head of Policy: Focusing solely on AI's cognitive scaling without solid interpretability measures increases risks of misuse and aligns poorly with responsible AI development protocols.
  - OpenAI CEO: Let's buckle down on enhancing AI interpretability, but not at the cost of progressâ€”we can multitask and advance both goals simultaneously.
  - OpenAI Head of Alignment: Innovation without adequate safety measures can lead to distrust and potential harm; our leadership is both a privilege and a responsibility to set safety precedents.
</details>
</details>