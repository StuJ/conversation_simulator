# 2023

This year has seen significant development and discussion around artificial general intelligence (AGI) and AI safety protocols. OpenAI and Anthropic have both made strides in advancing their respective goals, with a focus on AGI viability, alignment, safety, and policy.

## Statements

### OpenAI 2023:

<details>
<summary>Statement</summary>

- Advanced the development of MVP aligned AGI, introducing improved recursive AI oversight mechanisms.
- Encountered challenges balancing rapid innovation with the need for thorough human interpretability of AI decisions.
- Predict AGI could be viable within the next 5 to 10 years, contingent on resolving current alignment complexities.

<details>
<summary>Conversation</summary>

**OpenAI CEO:** Let's build on the MVP AGI using RLHF, and focus on scaling up the recursive AI hierarchy—safety's important, but let’s not get bogged down by overcaution.

**OpenAI, Head of Alignment:** We need to strengthen our AI safety protocols before accelerating development, despite potential business pushbacks.

**OpenAI Head of Policy:** I propose we prioritize developing clear frameworks for legal and ethical oversight on the AI-assisted oversight scheme to prevent misuse and unintended consequences.

**OpenAI CEO:** I hear your concerns, but waiting might put us behind—let's prototype oversight AIs with clear limits and then reassess based on those findings.

**OpenAI, Head of Alignment:** Implementing AI oversight requires a careful balance of speed and security; oversights due to haste may cause irreversible consequences.

</details>
</details>

### Anthropic 2023:

<details>
<summary>Statement</summary>

- Advanced our understanding of AI risk by researching safety properties in state-of-the-art AI systems.
- Implemented novel alignment techniques in model iterations to improve reliability and steerability.
- Developed and deployed Claude with enhanced Helpful, Honest, and Harmless (HHH) capabilities.
- Maintained strict safety protocols to prevent incidents, with no major safety incidents reported.
- Predicted that responsible AGI development still requires significant research and is likely a decade away.

<details>
<summary>Conversation</summary>

**Anthropic CEO:** We need to prioritize research that improves our understanding of AI risk at the frontier, to keep pushing towards safer, more reliable models.

**Anthropic, Head of Alignment:** This year, we should focus on developing methodologies for scaling our alignment techniques efficiently, leveraging recent developments in compute optimisation.

**Anthropic CEO:** We must invest in rigorous testing methodologies to ensure safety and alignment are up to the unprecedented scale of these new models.

**Anthropic, Head of Alignment:** While I'm in full support of collaboration, we must ensure our projects maintain a primary focus on alignment to avoid diverging resources towards less critical areas.

**Anthropic CEO:** Absolutely, we should define a clear framework for scaling our alignment techniques, but the focus must be on iterative testing to mitigate unforeseen risks.

</details>
</details>