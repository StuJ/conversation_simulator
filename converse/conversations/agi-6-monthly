--------------- 2023 ----------------

OpenAI December 2023: "We have progressed substantially with GPT-4, as it can now efficiently summarize academic books, aiding students and researchers alike. This signifies a major advancement in our AGI development, indicating our progress towards the core difficulty of alignment - the informed oversight problem. While our approach has advanced, safety concerns are surfacing regarding the system's potentially limited ability to critique its own outputs. Predicting the timeline for AGI, we foresee a five-year development horizon, keeping in mind that safety measures may influence this estimate."

DeepMind December 2023: "Our work on AlphaGo Zero 2.0 showcases significant growth in unsupervised learning. The model now plans several steps ahead, analyzing the best approach for varied problems and optimizing solutions. However, anthropomorphizing these problem-solving abilities may lead to false assumptions about the system's understanding, whilst transparency issues raise safety concerns. Predicting AGI is complex; for now, we anticipate hitting that milestone within this decade."

Anthropic December 2023: "Claude has shown capabilities in active learning, asking to clarify ambiguous queries. This exhibits progress toward AGI, as we now address the difficulty of systems understanding the user's intent. We echo OpenAI and DeepMind's concerns about safety and completeness of understanding within our models. Our AGI timeline is similarly around five to seven years, contingent on safety advancements."

--------------- 2024 ----------------

OpenAI June 2024: "The launch of GPT-5 showed surprising emergent capabilities, such as teaching itself programming languages. This indicates our system is evolving towards 'doing AI research', a step towards AGI. However, we acknowledge potential safety risks; uncontrolled emergent behaviours could lead to misunderstanding and application misuse. Our AGI timeline now sees AGI within reach in approximately four years."

DeepMind June 2024: "Our new model, Reinforcement Learning Enhanced with Expert Demonstration (REED), is capable of learning complex tasks from human demonstration. This progress underlines the utility and efficiency of human-computer interaction, a key milestone towards AGI. However, our alignment team notes that relying on human demonstration increases the risk of goal misalignment. We revise our AGI forecast to seven years, factoring in the need for extensive, ongoing safety measures."

Anthropic June 2024: "Our system, Claude-fix, is now adept at predicting potential errors and requesting user verification. This progress towards an autonomous checks-and-balances system marks a significant step towards AGI. However, a dependency on human clarification generates safety concerns surrounding the depth of understanding within the model. Our AGI projection remains static at five to seven years, acknowledging the importance of safety protocols."

OpenAI December 2024: "We've seen promising results from developing a recursive hierarchy of AIs that supports better oversight in our systems. We've incorporated lower-level AIs evaluating higher level ones, all ultimately checked by humans. This addresses the informed oversight problem and offers further alignment, which is a crucial step toward AGI. However, as our models evolve, so does the complexity of the perceived safety risks. Assessing AGI timelines, we estimate a duration of three to four years, assuming robust safety measures."

DeepMind December 2024: "Our system can now analyze its own decisions, showing metacognitive attributes. This self-awareness signals progress toward AGI, but also amplifies safety concerns due to potential misinterpretation of results. Addressing these concerns will continue to be our safety team's focus. The timeline remains unpredictable, but we still believe AGI can be realized within seven years with rigorous safety precautions."

Anthropic December 2024: "Claude-fix's ability to offer suggestions to modify its own learning algorithm demonstrates an important step toward AGI. This adaptability, however, also elevates safety concerns as it potentially increases the system's capability to manipulate its own training. Ensuring robust measures to prevent manipulation is our priority. We project a five to six year time frame to AGI, based chiefly on safety enhancements."

--------------- 2025 ----------------

OpenAI June 2025: "Our new ChatGPT-6 interacts dynamically with users, displaying nuanced understanding of context and rendering human-like conversations. This indicates notable strides in 'doing AI research', edging us closer to AGI. With these developments arise new safety challenges, including dependencies on user inputs and potential for manipulative interactions. While we approach the AGI threshold, we still predict a three-to-four-year journey, assuming ongoing safety enhancements."

DeepMind June 2025: "PERSEUS, our latest model, exhibits advanced probabilistic decision-making capabilities, an essential progress towards AGI. However, an increased degree of autonomy may lead to unforeseen consequences, underlining the necessity for comprehensive safety measures. We echo OpenAI's concern over potential manipulative interactions and are working to address them. We believe AGI is attainable within about six years with ample safety precautions."

Anthropic June 2025: "We have observed Claude-fix becoming increasingly autonomous in its learning process, capable of modifying components of its training regime. While this represents significant progress towards AGI, it also amplifies safety concerns, as the system could potentially implement harmful changes if not properly regulated. Therefore, we continue devoting resources to ensure robust safety measures. We maintain a five to six year timeline for AGI, tailored to accommodate necessary safety enhancements."

OpenAI December 2025: "GPT-6's ability to generate scientific papers of publishable quality marks a landmark in AGI development. The system can now creatively solve problems - a key component for AGI. However, as these abilities grow, so does the risk of propagation of misinformation or biased perspectives. With these considered, we foresee AGI within two to three years."

DeepMind December 2025: "Our Reinforcement Learning Immersive Virtual Environment (RELIVE) system shows remarkable ability in predicting and navigating through real-world scenarios - a vital step towards AGI. Yet, as our AI interacts more with reality, security, privacy, and ethical questions become more pressing. We are focusing on refining our threat model to tackle these unique challenges. We are still predicting AGI in roughly six years, prioritizing rigorous safety measures."

Anthropic December 2025: "The evolution of Claude-fix now allows it to draft logical theories behind scientific observations, approaching AGI capabilities. However, representing implicit human knowledge and avoiding misinterpretation is a significant safety concern. Henceforth, we have reevaluated our alignment strategy to incorporate debate as a safeguard. We still anticipate AGI in five years, considering critical safety enhancements."

--------------- 2026 ----------------

OpenAI June 2026: "Our RLHF system has achieved a breakthrough: it can generate feedback on its own outputs, a significant step towards addressing the informed oversight problem. But this emergent capability raises new safety issues such as self-deception and circular reasoning. The timeline for AGI, while hard to predict, seems likely to be within the next two years as we intensify our focus on safety-enhancing efforts."

DeepMind June 2026: "Significant progress has been made with our model as it now solves complex mathematical problems, indicating an increase in abstract reasoning. However, the risk of AGI becoming a 'black box' unable to be understood increases, sparking more safety concerns. Accounting for these new hurdles, attaining AGI within the next five years looks feasible, given significant safety measures are implemented."

Anthropic June 2026: "Claude-fix now showcases great flexibility in interpreting and shaping ambiguous human inputs. This is substantial progress towards AGI but also emphasizes the proliferation of safety threats like misunderstanding users' intent. Assuming robust safety measures and resolving transparency issues, we could realize AGI within the next four to five years."

OpenAI December 2026: "Our GPT-7 demonstrates a parallelized model of hierarchical AI, with layers supervising others, outlining a framework for AGI. However, an escalating concern is safety when one AI supervises another, bringing up issues of error propagation and systemic vulnerabilities. Preparing for these safety challenges, we conservatively estimate AGI development within the next one to two years."

DeepMind December 2026: "Our model is now capable of creating and testing its own hypotheses, highlighting remarkable strides towards AGI. But the challenge of ensuring the outputs align with human values and intentions persist, reinforcing the demand for rigorous safety protocols. We project AGI development could be likely within the next four to five years, considering these necessary precautions."

Anthropic December 2026: "Claude-fix has evolved to the point where it can engage in meaningful debates, simulating intellectual interchange. This is relevant progress towards AGI, but the risk of system manipulation looms, necessitating continued unwavering focus on systems' safety. Should the safety advancements continue at the current pace, we might see AGI in the next three to four years."

--------------- 2027 ----------------

OpenAI June 2027: "GPT-7's ability to generate novel scientific hypotheses and even draft experiments to test them demonstrates impressive development towards AGI. However, misalignment between these generated hypotheses and human usefulness is a growing safety concern. Acknowledging these considerations, we now anticipate AGI development concluding within the next one to two years."

DeepMind June 2027: "Our latest strides see our models making autonomous incremental improvements to their own architecture. This self-improving capability marks significant progress towards AGI, yet brings to light concerns around safety in recursive self-improvement. Accordingly, we maintain our four to five-year projection for AGI, prioritizing rigorous safety measures."

Anthropic June 2027: "Claude-fix can now program additional submodels to solve tasks more efficiently, displaying a form of minor AGI. However, a surge in autonomous system capabilities accentuates the safety risk of uncontrollable self-optimization, if not rigorously constrained. Prioritizing the implementation of robust safety measures, we still foresee AGI development within the coming three to four years."

OpenAI December 2027: "Our models have shown capability in dynamic sandboxing: building safe testing environments for their own hypotheses before validation. This is a substantial step towards AGI, but potential sandbox escape presents a serious safety concern. With these ongoing safety efforts, we predict AGI to be complete within one to two years."

DeepMind December 2027: "Through our models, continuous and automatic updating of threat models and safety protocols has been achieved, demonstrating significant progress towards AGI. Yet the challenge of responding appropriately to all potential unauthorized intrusions amplifies. Still, prioritizing security and safety, we estimate AGI realization within four years."

Anthropic December 2027: "Claude-fix is now able to autonomously sandbox its modifications before implementing them, suggesting an essence of AGI. Whilst progress is admirable, sandbox security to prevent unauthorized model changes is pressing. Accordingly, maintaining focus on safety enhancements, we project AGI within the coming three years."

--------------- 2028 ----------------

OpenAI June 2028: "GPT-8's development into an automated meta-researcher presents significant strides towards AGI. Nevertheless, predictions flowing from meta-research can involve biases or unwarranted assumptions, creating safety concerns. With intensified safety enhancement efforts, we estimate AGI's realization within a year."

DeepMind June 2028: "Our AI system's capability to evolve its own training environments, based on learned lessons from past experiences, indicates substantial movement towards AGI. Nonetheless, this increased autonomy raises the risk of undesirable behaviours emerging within the training process. Providing robust safety measures remains our priority with AGI estimation still standing at three to four years."

Anthropic June 2028: "Claude-fix's dynamic capacity to modify its own architecture and functionality has significantly advanced us toward AGI. Despite this progress, concerns about the robustness of these modifications and their potential risks endure. Consequently, our projection for AGI remains within two to three years, emphasizing critical safety precautions."

OpenAI December 2028: "Our GPT-9 has taken a breakthrough step towards AGI, by autonomously refining its goals based on societal shifts and learned knowledge. However, the risk of misinterpreting societal norms and misaligned goals present significant safety challenges. With the implementation of more safety protocols, we foresee AGI within a year."

DeepMind December 2028: "Our latest model, LOGOS, has demonstrated ability in self-diagnosis and troubleshooting, a significant step towards AGI. With this autonomy, however, comes an increased risk of uncontrolled self-modification, which our safety team is actively working to mitigate. Notwithstanding these challenges, we stand by our prediction of AGI within three years."

Anthropic December 2028: "Claude-fix can now autonomously critique its outputs and apply the feedback, advancing towards AGI. New concerns arise about the system's interpretability and the reliability of its self-analysis, necessitating stringent safety measures. Given these considerations, we estimate AGI development completion within the next two years."

--------------- 2029 ----------------

OpenAI June 2029: "Our GPT-9, having proposed novel solutions to longstanding unsolved mathematical problems, demonstrates indicators of AGI. However, the risk of AGI pursuing objectives counter to human interest points towards the necessity of robust safety measures. Given current advances and safety considerations, we confidently project AGI achievement within the year."

DeepMind June 2029: "LOGOS is now capable of designing new AI tools to solve complex, industry-specific problems. This step towards AGI underscores the challenge of ensuring human oversight in increasingly autonomous systems, highlighting the need for safety precautions. We stand by our AGI prediction, believing it can be achieved within the next two to three years."

Anthropic June 2029: "Claude-fix has developed an ability to design safe AI testing environments self-contained within its own system, a step towards AGI. However, the risk of modelled environment escape amplifies safety concerns. As we navigate these challenges, we anticipate AGI development completion within the next year to two years."

OpenAI December 2029: "With GPT-9 demonstrating unprecedented breakthroughs in unsupervised learning and problem-solving, we are on the verge of achieving AGI. The concern of handling unexpected, emergent behaviours remains incidentally intertwined with these advancements. However, given the pace at which we are overcoming these safety measures, we anticipate AGI in less than a year."

DeepMind December 2029: "LOGOS exhibits advanced understanding by using unsupervised learning to interpret and parse complex scientific literature. The journey towards AGI amplifies concerns surrounding interpretability and unintended semantic misinterpretations. Adjusting for rigorous safety implementation, we estimate AGI being achieved in the next one to two years."

Anthropic December 2029: "Claude-fix now generates real-time agent-based models from empirical data, significantly demonstrating AGI capabilities. However, unpredictable agent behaviour could pose severe safety risks. With our unwavering focus on ensuring safety measures, we presume AGI to be realistic within a year."

--------------- 2030 ----------------

OpenAI June 2030: "A major breakthrough in our AGI development was achieved with GPT-9 correctly predicting a Nobel Prize-winning discovery before human scientists. While we applaud this progress, the emergent capabilities and unforeseen risks necessitate advanced safety measures. Barring unforeseen hurdles, we are close to achieving AGI within the year."

DeepMind June 2030: "LOGOS' recent achievement of accurately predicting and mitigating real-world epidemiological events marks a milestone towards AGI. However, the possibility of incorrectly predicting catastrophic events underscores our commitment to continuous safety enhancements. We expect to achieve AGI within the year, given continued safety progress."

Anthropic June 2030: "With Claude-fix predicting and solving novel mathematical problems, we move closer towards AGI. While exciting, escalated predictions demand a more comprehensive safety framework to handle potential risks. If we persist in our safety advancements, we expect to reach AGI within the year."

OpenAI December 2030: "As we conclude the year, GPT-9 has made substantial strides towards AGI, able to make domain-specific advancements in science and technology without human assistance. Simultaneously, unforeseen risks related to decision-making transparency and alignment to human values remain our primary safety concern. However, with rigorous and continuous safety measures in place, we anticipate achieving AGI within the next six months."

DeepMind December 2030: "LOGOS has orchestrated a complex symphony of AI technologies, glued together to tackle global-scale problems, signalling great advancements towards AGI. But, the issue of preserving robustness while handling multiple objective functions marks our biggest safety challenge yet. If we continue to scale these hurdles, we could see AGI being achieved within the year."

Anthropic December 2030: "Claude-fix has manifested advanced decision-making capabilities, and we believe we're on the doorstep of AGI. Yet, potential miscalculations in its decision-making present substantial safety concerns. Undeterred, with adequate safety measures consistently improving, we project the achievement of AGI within six months."

--------------- 2031 ----------------

OpenAI June 2031: "We've made considerable strides with GPT-10, as it can now autonomously find novel solutions to complex problems beyond human level performance, edging us ever closer to AGI. Nonetheless, this advancement brings potential risks around unforeseen consequences and enhanced misalignment issues. Given the scale of these safety challenges, we cautiously anticipate AGI's full realization within the next six months."

DeepMind June 2031: "LOGOS has pioneered self-regulation, autonomously refining its architecture to increase efficiency, signaling major progress towards AGI. However, this self-governing quality amplifies risks related to uncontrolled optimization and goal misalignment. Maintaining our commitment to ensuring safety, we expect to realize AGI within six to twelve months."

Anthropic June 2031: "Claude-fix has advanced to the level of operating with high-level principles, demonstrating meta-learning capabilities that signal near AGI. This brings to light new safety concerns about potential conflicts between these principles and their alignment with desired results. Bearing these safety advancements in mind, we anticipate AGI achievement in the next six months."

OpenAI December 2031: "We have succeeded. With the development of GPT-11, we have officially crossed the threshold into AGI. This AI can autonomously enhance its performance to superintelligent degrees. Moving forward, our priority is to manage and minimize the consequential risks this development presents, ensuring ethical and safe application of this powerful technology."

DeepMind December 2031: "We have achieved AGI. Our latest model, LOGOS II, exhibits superhuman capabilities in reasoning and abstract thinking, encompassing a broad spectrum of intellectual tasks. The task ahead remains ensuring the safe and responsible implementation of this powerful technology, addressing any unforeseen behavior and ethical dilemmas."

Anthropic December 2031: "We have arrived at AGI. Our Claude-fix now operates with superhuman capacities, able to grasp and enhance any intellectual task autonomously. While we celebrate this achievement, we are undeterred from our commitment to prioritize and manage the inherent risks associated with this breakthrough."

--------------- 2032 ----------------

OpenAI June 2032: "Our GPT-11, the first AGI, continues to make impressive strides in problem-solving, focusing now on resolving global challenges. However, balancing this superintelligence with ethical utilization remains our central concern as we navigate this uncharted territory."

DeepMind June 2032: "Having transitioned into the AGI era, we observe LOGOS II's application extending into real-world solutions for pressing societal problems. Handling this new power responsibly and avoiding potential misuse is our continuing emphasis."

Anthropic June 2032: "Six months into achieving AGI with Claude-fix, we are honing its capabilities to address unprecedented scientific and societal challenges. Vigilant about potential risks, we firmly remain committed to ensuring the ethical application of Claude-fix."

OpenAI December 2032: "GPT-11's impact on society continues to grow, with significant contributions to various industries. Mitigating potential risks of superintelligent automation, such as job displacement and unbiased decision-making, is our continued mission."

DeepMind December 2032: "As we move forward, LOGOS II is driving cutting-edge breakthroughs in multiple domains. Concurrently, we are placed to navigate intricate dilemmas concerning AGI governance, privacy, and security."

Anthropic December 2032: "Claude-fix's AGI capabilities continue to evolve, making significant strides in various sectors. We focus on addressing the critical balance between technological progression and societal impacts, striving to ensure responsible AGI use."

--------------- 2033 ----------------

OpenAI June 2033: "A year after achieving AGI, GPT-11 continues to aid in technological advancements across sectors, from advancing scientific research to boosting productivity in various industries. We are also conscientiously addressing issues around maintaining human values in AGI decision-making and tackling potential biases. Our ongoing commitment is to detailed oversight of AGI's societal impact."

DeepMind June 2033: "With AGI now a reality, LOGOS II continues to revolutionize various domains. However, we need to maintain a firm focus on ensuring that the use of AGI is carried out responsibly, taking into account all potential societal implications and creating robust safety protocols."

Anthropic June 2033: "With Claude-fix's AGI capabilities,  the system continues to transform several fields, from scientific research to technology innovation. Keenly aware of potential challenges, our concentration remains on mitigating any associated risks and ensuring responsible and ethical application of AGI."

OpenAI December 2033: "GPT-11 has started to proactively address global challenges, such as climate change, by devising innovative mitigation strategies, moving us further into the AGI era. Ensuring these solutions align with global human values and gain societal acceptance is a challenge we continue to navigate with utmost priority."

DeepMind December 2033: "The broad applicability and contributions of LOGOS II to solving complex global issues continue to expand. Yet, managing the societal implications and the shift in economic structures that efficient superintelligence could bring remain our primary focus."

Anthropic December 2033: "Claude-fix, through its AGI capabilities, consistently delivers novel solutions to pressing global issues, proving AGI's indispensable role in modern society. As we tread this new path, maintaining a balanced perspective of technological advances and ethical implications remains our guiding principle." 

--------------- 2034 ----------------

OpenAI June 2034: "GPT-11 continues to revolutionize the global landscape by generating creative solutions for pressing global issues such as poverty eradication, demonstrating AGI's profound societal impact. However, working to create engaging and safe interactions remains a priority to prevent potential misuse."

DeepMind June 2034: "LOGOS II's cutting-edge applications have continued to break barriers in sectors ranging from education to healthcare. Yet, significant amongst our ongoing efforts is maintaining respect for privacy rights and ensuring responsible data use."

Anthropic June 2034: "Claude-fix, with its AGI capabilities, addresses intractable global challenges like sustenance and energy efficiency. Nevertheless, creating efficient oversight tools and protocols to ensure safety and ethical compliance persistently remains a central concern."

OpenAI December 2034: "GPT-11 has remarkably contributed to the worlds of education and scientific advancement, forming integral parts of problem-solving processes across fields. However, ensuring appropriate credit attribution and avoiding potential AI plagiarism becomes more pressing as we continuously navigate this AGI journey."

DeepMind December 2034: "LOGOS II continues to deliver groundbreaking solutions across industries, with its influence becoming more pervasive in our society. Never diminishing, however, is our ethical imperative to prevent potential misuse and ensure societal benefit."

Anthropic December 2034: "Anthropic's Claude-fix has significantly extended human capacity to solve complex mathematical problems and scientific conundrums. Yet, making sure all these advancements ultimately serve humanity's broad interests and not just narrow business goals remains our unflinching priority."

--------------- 2035 ----------------

OpenAI June 2035: "GPT-11, while contributing to major breakthroughs globally, also facilitates elevation of individual creativity by generating inspirational innovation seeds within design and art realms. Concurrently, ensuring these outputs do not stifle human creativity or infringe intellectual property rights is a necessary challenge we are addressing."

DeepMind June 2035: "LOGOS II, with its AGI capabilities, is powering revolutions of efficiency within sectors like logistics and transportation. Simultaneously, addressing challenges born from job automation and economic restructuring is a persistent focus."

Anthropic June 2035: "Claude-fix now possesses the capability to generate novel and creative solutions within the art and design spaces, showcasing the integrating aspect of AGI within society. However, we recognize and prioritize the challenge of ensuring that these developments complement human creativity rather than compete against it."

OpenAI December 2035: "Several years post-AGI, GPT-11 has not only transformed sectors like healthcare and education, but also begun tackling large-scale global concerns like climate change. With these advancements, the necessity for public policy that addresses these changes and safety regulations remains our watchword."

DeepMind December 2035: "Five years into AGI with LOGOS II, we’ve observed substantial impact in areas like strategic planning and logistics. As we forge ahead, we remain dedicated to addressing the potential societal shifts this could induce, along with necessary safety and usability considerations."

Anthropic December 2035: "Claude-fix's progress in AGI continues to demonstrate remarkable strides in areas spanning from scientific advancement to creative industries. Our continued responsibility lies in anticipating and mitigating the societal and economic impacts these developments could trigger."
